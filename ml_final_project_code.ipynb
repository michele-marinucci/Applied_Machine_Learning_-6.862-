{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains the code for:\n",
    "- preliminary handling of missing values \n",
    "- engineering and selcting features \n",
    "- running the Random Forest model \n",
    "- performing a grid search over possible ways to handle missing values\n",
    "- performing a grid search over random forest parameters \n",
    "- computing state of the art result with RF\n",
    "- performing a grid search over decision tree, k-nearest neighbor and logistic regression parameters\n",
    "- creating an ensemble of hypertuned models\n",
    "- training and performing cross validation on an autosklearn model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>STOCK</th>\n",
       "      <th>INDUSTRY</th>\n",
       "      <th>INDUSTRY_GROUP</th>\n",
       "      <th>SECTOR</th>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <th>RET_1</th>\n",
       "      <th>VOLUME_1</th>\n",
       "      <th>RET_2</th>\n",
       "      <th>VOLUME_2</th>\n",
       "      <th>...</th>\n",
       "      <th>VOLUME_16</th>\n",
       "      <th>RET_17</th>\n",
       "      <th>VOLUME_17</th>\n",
       "      <th>RET_18</th>\n",
       "      <th>VOLUME_18</th>\n",
       "      <th>RET_19</th>\n",
       "      <th>VOLUME_19</th>\n",
       "      <th>RET_20</th>\n",
       "      <th>VOLUME_20</th>\n",
       "      <th>RET</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>-0.015748</td>\n",
       "      <td>0.147931</td>\n",
       "      <td>-0.015504</td>\n",
       "      <td>0.179183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630899</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>-0.379412</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>-0.110597</td>\n",
       "      <td>-0.012959</td>\n",
       "      <td>0.174521</td>\n",
       "      <td>-0.002155</td>\n",
       "      <td>-0.000937</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>104</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.090580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.018518</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.028777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.034722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>142</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.096282</td>\n",
       "      <td>-0.058896</td>\n",
       "      <td>0.084771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010336</td>\n",
       "      <td>-0.017612</td>\n",
       "      <td>-0.354333</td>\n",
       "      <td>-0.006562</td>\n",
       "      <td>-0.519391</td>\n",
       "      <td>-0.012101</td>\n",
       "      <td>-0.356157</td>\n",
       "      <td>-0.006867</td>\n",
       "      <td>-0.308868</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031298</td>\n",
       "      <td>-0.429540</td>\n",
       "      <td>0.007756</td>\n",
       "      <td>-0.089919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012105</td>\n",
       "      <td>0.033824</td>\n",
       "      <td>-0.290178</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>-0.663834</td>\n",
       "      <td>-0.013520</td>\n",
       "      <td>-0.562126</td>\n",
       "      <td>-0.036745</td>\n",
       "      <td>-0.631458</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>-0.847155</td>\n",
       "      <td>-0.039302</td>\n",
       "      <td>-0.943033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277083</td>\n",
       "      <td>-0.012659</td>\n",
       "      <td>0.139086</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>-0.017547</td>\n",
       "      <td>0.004256</td>\n",
       "      <td>0.579510</td>\n",
       "      <td>-0.040817</td>\n",
       "      <td>0.802806</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    DATE  STOCK  INDUSTRY  INDUSTRY_GROUP  SECTOR  SUB_INDUSTRY     RET_1  \\\n",
       "ID                                                                          \n",
       "0      0      2        18               5       3            44 -0.015748   \n",
       "1      0      3        43              15       6           104  0.003984   \n",
       "2      0      4        57              20       8           142  0.000440   \n",
       "3      0      8         1               1       1             2  0.031298   \n",
       "4      0     14        36              12       5            92  0.027273   \n",
       "\n",
       "    VOLUME_1     RET_2  VOLUME_2  ...  VOLUME_16    RET_17  VOLUME_17  \\\n",
       "ID                                ...                                   \n",
       "0   0.147931 -0.015504  0.179183  ...   0.630899  0.003254  -0.379412   \n",
       "1        NaN -0.090580       NaN  ...        NaN  0.003774        NaN   \n",
       "2  -0.096282 -0.058896  0.084771  ...  -0.010336 -0.017612  -0.354333   \n",
       "3  -0.429540  0.007756 -0.089919  ...   0.012105  0.033824  -0.290178   \n",
       "4  -0.847155 -0.039302 -0.943033  ...  -0.277083 -0.012659   0.139086   \n",
       "\n",
       "      RET_18  VOLUME_18    RET_19  VOLUME_19    RET_20  VOLUME_20    RET  \n",
       "ID                                                                        \n",
       "0   0.008752  -0.110597 -0.012959   0.174521 -0.002155  -0.000937   True  \n",
       "1  -0.018518        NaN -0.028777        NaN -0.034722        NaN   True  \n",
       "2  -0.006562  -0.519391 -0.012101  -0.356157 -0.006867  -0.308868  False  \n",
       "3  -0.001468  -0.663834 -0.013520  -0.562126 -0.036745  -0.631458  False  \n",
       "4   0.004237  -0.017547  0.004256   0.579510 -0.040817   0.802806  False  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = pd.read_csv('./x_train.csv', index_col='ID')\n",
    "y_train = pd.read_csv('./y_train.csv', index_col='ID')\n",
    "train = pd.concat([x_train, y_train], axis=1)\n",
    "test = pd.read_csv('./x_test.csv', index_col='ID')\n",
    "train_original = train\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(grouping = [\"INDUSTRY_GROUP\"],k=8,train=train_original):\n",
    "    returns = ['RET_%d' % (i + 1) for i in range(20)]\n",
    "    volume = ['VOLUME_%d' % (i + 1) for i in range(20)]\n",
    "    \n",
    "    #Selecting data to fill horizontally based on threshold\n",
    "    mask1 = (train[returns].isna().astype(\"int\").sum(axis=1)<k)\n",
    "    mask2 = (train[volume].isna().astype(\"int\").sum(axis=1)<k)\n",
    "    train1 = train[np.logical_and(mask1,mask2)]\n",
    "    \n",
    "    #Filling data horizontally \n",
    "    train1[returns] = train1[returns].T.fillna(train1[returns].mean(axis=1)).T\n",
    "    train1[volume] = train1[volume].T.fillna(train1[volume].mean(axis=1)).T\n",
    "    \n",
    "    #Selecting other part of data\n",
    "    train2 = train.drop(train1.index,axis=0)\n",
    "    df_obj = (train.groupby(grouping)[returns].transform(\"mean\")).loc[train2.index]\n",
    "    mask = train2[returns].isnull()\n",
    "    df_vol = train2[returns]\n",
    "    df_vol[mask.eq(True)] = df_obj\n",
    "    \n",
    "    #Filling returns vertically based on parameter \"grouping\"\n",
    "    train2[returns] = df_vol\n",
    "    df_obj = (train.groupby(grouping)[volume].transform(\"mean\")).loc[train2.index]\n",
    "    mask = train2[volume].isnull()\n",
    "    df_vol = train2[volume]\n",
    "    df_vol[mask.eq(True)] = df_obj\n",
    "    \n",
    "    #Filling volume vertically based on parameter \"grouping\"\n",
    "    train2[volume] = df_vol\n",
    "    train = pd.concat([train1,train2],axis=0).sort_values(by = [\"ID\"])\n",
    "    train = train.fillna(0)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(train):\n",
    "    \n",
    "    #Creating absolute value returns and summing them\n",
    "    for i in range(20):\n",
    "        train[\"abs_RET_%d\" % (i+1)] = np.abs(train[\"RET_%d\" % (i+1)])\n",
    "    train[\"accumulate_abs_returns_{}\".format(20)] = np.sum([train[\"abs_RET_%d\" % (i+1)] for i in range(20)], axis=0)\n",
    "\n",
    "    #Creating Log RET and summing them\n",
    "    for i in range(20):\n",
    "        train[\"log_RET_%d\" % (i+1)] = np.log(1 + train[\"RET_%d\" % (i+1)])\n",
    "    train[\"accumulate_log_returns_20\"] = np.sum([train[\"log_RET_%d\" % (i+1)] for i in range(20)], axis=0)\n",
    "    \n",
    "    #Creating BETA\n",
    "    def Beta(x):\n",
    "        y = x[returns].values.reshape(-1,1)\n",
    "        X = x[[\"RET_%s_mkt\"%(i+1) for i in range(20)]].values.reshape(-1,1)\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(X,y)\n",
    "        return reg.coef_[0][0]\n",
    "    returns = ['RET_%d' % (i + 1) for i in range(20)]\n",
    "    volume = ['VOLUME_%d' % (i + 1) for i in range(20)]\n",
    "    average_returns_over_dates = train.groupby(\"DATE\")[returns].transform(\"mean\")\n",
    "    average_returns_over_dates.columns = [\"RET_%s_mkt\"%(i+1) for i in range(20)]\n",
    "    train[\"market_beta\"] = pd.concat((average_returns_over_dates,train[returns]),axis=1).apply(Beta,axis=1)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(train,beta = False):\n",
    "    \n",
    "    new_features = []\n",
    "\n",
    "    new_features += [\"accumulate_abs_returns_20\"]\n",
    "    new_features += [\"accumulate_log_returns_20\"]\n",
    "    \n",
    "    if beta:\n",
    "        new_features += [\"market_beta\"]\n",
    "\n",
    "\n",
    "    # Conditional mean on INDUSTRY_GROUP and DATE for RET_1 and VOLUME_1 \n",
    "    shifts = [1]\n",
    "    statistics = ['mean']  \n",
    "    gb_features = ['INDUSTRY_GROUP', 'DATE']\n",
    "    target_feature_list = ['RET',\"VOLUME\"]\n",
    "    tmp_name = '_'.join(gb_features)\n",
    "    for target_feature in target_feature_list:\n",
    "        for shift in shifts:\n",
    "            for stat in statistics:\n",
    "                name = f'{target_feature}_{shift}_{tmp_name}_{stat}'\n",
    "                feat = f'{target_feature}_{shift}'\n",
    "                new_features.append(name)\n",
    "                for data in [train]:\n",
    "                    data[name] = data.groupby(gb_features)[feat].transform(stat)\n",
    "\n",
    "        target = 'RET'\n",
    "\n",
    "    n_shifts_ret = 5\n",
    "    n_shifts_vol = 5 # If you don't want all the shifts to reduce noise\n",
    "    features = ['RET_%d' % (i + 1) for i in range(n_shifts_ret)]\n",
    "    features += ['VOLUME_%d' % (i + 1) for i in range(n_shifts_vol)]\n",
    "    features += new_features  # The conditional features\n",
    "                    \n",
    "    return train, features, target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building & 4-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RF(train,test, features,target,n_estimator=300, depth=8):\n",
    "    \n",
    "    X_train = train[features]\n",
    "    y_train = train[target]\n",
    "    \n",
    "    #Parameters of the Random Forest\n",
    "\n",
    "    rf_params = {\n",
    "        'n_estimators': n_estimator,\n",
    "        'max_depth': depth,\n",
    "        'random_state': 0,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    \n",
    "    #The below is concerned with making a 4K cross-validation BY DATE!\n",
    "    \n",
    "    train_dates = train['DATE'].unique()\n",
    "    test_dates = test['DATE'].unique()\n",
    "\n",
    "    n_splits = 4\n",
    "    scores = []\n",
    "    models = []\n",
    "\n",
    "    splits = KFold(n_splits=n_splits, random_state=0,\n",
    "                   shuffle=True).split(train_dates)\n",
    "\n",
    "    for i, (local_train_dates_ids, local_test_dates_ids) in enumerate(splits):\n",
    "        local_train_dates = train_dates[local_train_dates_ids]\n",
    "        local_test_dates = train_dates[local_test_dates_ids]\n",
    "\n",
    "        local_train_ids = train['DATE'].isin(local_train_dates)\n",
    "        local_test_ids = train['DATE'].isin(local_test_dates)\n",
    "\n",
    "        X_local_train = X_train.loc[local_train_ids]\n",
    "        y_local_train = y_train.loc[local_train_ids]\n",
    "        X_local_test = X_train.loc[local_test_ids]\n",
    "        y_local_test = y_train.loc[local_test_ids]\n",
    "\n",
    "        X_local_train = X_local_train.fillna(0)\n",
    "        X_local_test = X_local_test.fillna(0)\n",
    "\n",
    "        model = RandomForestClassifier(**rf_params)\n",
    "        model.fit(X_local_train, y_local_train)\n",
    "\n",
    "        y_local_pred = model.predict_proba(X_local_test)[:, 1]\n",
    "\n",
    "        sub = train.loc[local_test_ids].copy()\n",
    "        sub['pred'] = y_local_pred\n",
    "        y_local_pred = sub.groupby('DATE')['pred'].transform(lambda x: x > x.median()).values\n",
    "\n",
    "        models.append(model)\n",
    "        score = accuracy_score(y_local_test, y_local_pred)\n",
    "        scores.append(score)\n",
    "        print(f\"Fold {i+1} - Accuracy: {score* 100:.2f}%\")\n",
    "\n",
    "    mean = np.mean(scores)*100\n",
    "    std = np.std(scores)*100\n",
    "    u = (mean + std)\n",
    "    l = (mean - std)\n",
    "    print(f'Accuracy: {mean:.2f}% [{l:.2f} ; {u:.2f}] (+- {std:.2f})')\n",
    "    \n",
    "    return mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Accuracy: 52.24%\n",
      "Fold 2 - Accuracy: 50.40%\n",
      "Fold 3 - Accuracy: 50.87%\n",
      "Fold 4 - Accuracy: 52.65%\n",
      "Accuracy: 51.54% [50.61 ; 52.47] (+- 0.93)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51.54114080164959"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = missing_values(grouping=[\"SECTOR\",\"DATE\"],k=15)\n",
    "train = feature_engineering(train)\n",
    "train,features,target = feature_selection(train,beta = True)\n",
    "run_RF(train,test, features,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NA Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given our conditioning on SECTOR and DATE, iterating over best k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sector_date_k = {}\n",
    "for elt in range(15):\n",
    "    train = missing_values(k = elt)\n",
    "    train = feature_engineering(train)\n",
    "    train,features,target = feature_selection(train)\n",
    "    result_sector_date_k[elt] = run_RF(train,test, features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "pd.Series(result_sector_date_k).plot()\n",
    "ax.set_title(\"k Grid Search \")\n",
    "ax.set_xlabel(\"K\")\n",
    "ax.set_ylabel(\"Accuracy (%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given optimal k, iterating over pairs of feature + DATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = [[\"SECTOR\",\"DATE\"],[\"INDUSTRY\",\"DATE\"],[\"SUB_INDUSTRY\",\"DATE\"],[\"INDUSTRY_GROUP\",\"DATE\"]]\n",
    "result_group_with_date = {}\n",
    "for elt in group:\n",
    "    train = missing_values(grouping = elt)\n",
    "    train = feature_engineering(train)\n",
    "    train,features,target = feature_selection(train)\n",
    "    result_group_with_date[elt] = run_RF(train,test, features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "pd.Series(result_group_with_date).plot.barh()\n",
    "plt.xlim(51.5,52)\n",
    "ax.set_title(\"Grid Search Na Values - Grouping by parameters & Dates \")\n",
    "ax.set_ylabel(\"Group By Parameter\")\n",
    "ax.set_xlabel(\"Accuracy (%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given optimal k, iterating over single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = [\"SECTOR\",\"INDUSTRY\",\"SUB_INDUSTRY\",\"INDUSTRY_GROUP\",\"DATE\"]\n",
    "result_group_without_date = {}\n",
    "for elt in group:\n",
    "    train = missing_values(grouping = [elt])\n",
    "    train = feature_engineering(train)\n",
    "    train,features,target = feature_selection(train)\n",
    "    result_group_without_date[elt] = run_RF(train,test, features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "pd.Series(result_group_without_date).plot.barh()\n",
    "plt.xlim(51.5,52)\n",
    "ax.set_title(\"Grid Search Na Values - Grouping by parameters\")\n",
    "ax.set_ylabel(\"Group By Parameter\")\n",
    "ax.set_xlabel(\"Accuracy (%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given optimal conditioning on feature, re-search optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_industry_group_k = {}\n",
    "for elt in range(15):\n",
    "    train = missing_values(k = elt)\n",
    "    train = feature_engineering(train)\n",
    "    train,features,target = feature_selection(train)\n",
    "    result_industry_group_k[elt] = run_RF(train,test, features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "pd.Series(result_industry_group_k).plot()\n",
    "ax.set_title(\"New k Grid Search \")\n",
    "ax.set_xlabel(\"K\")\n",
    "ax.set_ylabel(\"Accuracy (%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweaking number of trees\n",
    "grid_RF = {}\n",
    "for i in n_estimator:\n",
    "    grid_RF[i] = run_RF(train,test, features,target,n_estimator = i, depth=2**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "pd.Series(grid_RF).plot()\n",
    "ax.set_title(\"RF - n_estimators - Grid Search\")\n",
    "ax.set_xlabel(\"Number of Trees\")\n",
    "ax.set_ylabel(\"Accuracy (%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweaking depth\n",
    "grid_RF_depth = {}\n",
    "for i in depth:\n",
    "    grid_RF_depth[i] = run_RF(train,test, features,target,depth=i,n_estimator=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "pd.Series(grid_RF_depth).plot()\n",
    "ax.set_title(\"RF - depth - estimator Grid Search\")\n",
    "ax.set_xlabel(\"Number of Trees\")\n",
    "ax.set_ylabel(\"Accuracy (%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Accuracy: 52.70%\n",
      "Fold 2 - Accuracy: 50.76%\n",
      "Fold 3 - Accuracy: 51.12%\n",
      "Fold 4 - Accuracy: 52.94%\n",
      "Accuracy: 51.88% [50.93 ; 52.84] (+- 0.95)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51.88274677375793"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = missing_values()\n",
    "train = feature_engineering(train)\n",
    "train,features,target = feature_selection(train)\n",
    "run_RF(train,test, features,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Estimation (Other Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#building all models needed\n",
    "kNN = KNeighborsClassifier()\n",
    "NB = GaussianNB()\n",
    "LR = LogisticRegression()\n",
    "DT = DecisionTreeClassifier()\n",
    "RF = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of hyperparameters to iterate over\n",
    "DT_parameter_grid = {'criterion':('gini', 'entropy'), 'min_samples_leaf':list(range(3,12)), 'max_depth':list(range(4,12)), 'max_features':list(range(3,10))}\n",
    "LR_parameter_grid = {'max_iter':list(range(30,50)), 'C': [0.01, 0.05, 0.1, 0.5, 1,2], 'n_jobs': [-1], 'penalty': ('l1', 'l2')}\n",
    "kNN_parameter_grid = {'n_neighbors':list(range(2,15)), 'weights':('uniform', 'distance'), 'leaf_size':list(range(10,30)), 'n_jobs': [-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# creating parameter grid for each models hyperparameters\n",
    "LR_grid = ParameterGrid(LR_parameter_grid)\n",
    "DT_grid = ParameterGrid(DT_parameter_grid)\n",
    "kNN_grid = ParameterGrid(kNN_parameter_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Hyperparameter Tuning\n",
    "def DT_hyper_param(param_grid):\n",
    "    '''trains hyperparameters on a specific model, returns list of accuracy values'''\n",
    "    all_scores = []\n",
    "    for params in param_grid:\n",
    "        hyp_model = DecisionTreeClassifier(criterion = params['criterion'], min_samples_leaf = params['min_samples_leaf'], \n",
    "                                            max_depth = params['max_depth'],max_features = params['max_features'])\n",
    "        \n",
    "        X_train = train[features]\n",
    "        y_train = train[target]\n",
    "        \n",
    "        train_dates = train['DATE'].unique()\n",
    "        #test_dates = test['DATE'].unique()\n",
    "\n",
    "        n_splits = 4\n",
    "        scores = []\n",
    "        models = []\n",
    "\n",
    "        splits = KFold(n_splits=n_splits, random_state=0,\n",
    "                       shuffle=True).split(train_dates)\n",
    "\n",
    "        for i, (local_train_dates_ids, local_test_dates_ids) in enumerate(splits):\n",
    "            local_train_dates = train_dates[local_train_dates_ids]\n",
    "            local_test_dates = train_dates[local_test_dates_ids]\n",
    "\n",
    "            local_train_ids = train['DATE'].isin(local_train_dates)\n",
    "            local_test_ids = train['DATE'].isin(local_test_dates)\n",
    "\n",
    "            X_local_train = X_train.loc[local_train_ids]\n",
    "            y_local_train = y_train.loc[local_train_ids]\n",
    "            X_local_test = X_train.loc[local_test_ids]\n",
    "            y_local_test = y_train.loc[local_test_ids]\n",
    "\n",
    "            X_local_train = X_local_train.fillna(0)\n",
    "            X_local_test = X_local_test.fillna(0)\n",
    "\n",
    "            hyp_model.fit(X_local_train, y_local_train)\n",
    "\n",
    "            y_local_pred = hyp_model.predict_proba(X_local_test)[:, 1]\n",
    "\n",
    "            sub = train.loc[local_test_ids].copy()\n",
    "            sub['pred'] = y_local_pred\n",
    "            y_local_pred = sub.groupby('DATE')['pred'].transform(lambda x: x > x.median()).values\n",
    "\n",
    "            score = accuracy_score(y_local_test, y_local_pred)\n",
    "            scores.append(score)\n",
    "            print(f\"Fold {i+1} - Accuracy: {score* 100:.2f}%\")\n",
    "\n",
    "        all_scores.append(np.mean(scores)*100)\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_acc_scores = DT_hyper_param(DT_grid)\n",
    "\n",
    "# optimal DT model\n",
    "DT_grid[np.argmax(DT_acc_scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN Hyperparameter Tuning\n",
    "def kNN_hyper_param(param_grid):\n",
    "    '''trains hyperparameters on a specific model, returns list of accuracy values'''\n",
    "    all_scores = []\n",
    "    for params in param_grid:\n",
    "        hyp_model = KNeighborsClassifier(n_neighbors = params['n_neighbors'], weights = params['weights'], n_jobs = params['n_jobs'], leaf_size = params['leaf_size'])\n",
    "        \n",
    "        X_train = train.iloc[:,:16] \n",
    "        y_train = train.iloc[:,-1]\n",
    "        \n",
    "        train_dates = train['DATE'].unique()\n",
    "        #test_dates = test['DATE'].unique()\n",
    "\n",
    "        n_splits = 4\n",
    "        scores = []\n",
    "        models = []\n",
    "\n",
    "        splits = KFold(n_splits=n_splits, random_state=0,\n",
    "                       shuffle=True).split(train_dates)\n",
    "\n",
    "        for i, (local_train_dates_ids, local_test_dates_ids) in enumerate(splits):\n",
    "            local_train_dates = train_dates[local_train_dates_ids]\n",
    "            local_test_dates = train_dates[local_test_dates_ids]\n",
    "\n",
    "            local_train_ids = train['DATE'].isin(local_train_dates)\n",
    "            local_test_ids = train['DATE'].isin(local_test_dates)\n",
    "\n",
    "            X_local_train = X_train.loc[local_train_ids]\n",
    "            y_local_train = y_train.loc[local_train_ids]\n",
    "            X_local_test = X_train.loc[local_test_ids]\n",
    "            y_local_test = y_train.loc[local_test_ids]\n",
    "\n",
    "            X_local_train = X_local_train.fillna(0)\n",
    "            X_local_test = X_local_test.fillna(0)\n",
    "\n",
    "            hyp_model.fit(X_local_train, y_local_train)\n",
    "\n",
    "            y_local_pred = hyp_model.predict_proba(X_local_test)[:, 1]\n",
    "\n",
    "            sub = train.loc[local_test_ids].copy()\n",
    "            sub['pred'] = y_local_pred\n",
    "            y_local_pred = sub.groupby('DATE')['pred'].transform(lambda x: x > x.median()).values\n",
    "\n",
    "            score = accuracy_score(y_local_test, y_local_pred)\n",
    "            scores.append(score)\n",
    "            print(f\"Fold {i+1} - Accuracy: {score* 100:.2f}%\")\n",
    "\n",
    "        all_scores.append(np.mean(scores)*100)\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kNN_acc_scores = kNN_hyper_param(kNN_grid)\n",
    "\n",
    "# optimal kNN model\n",
    "kNN_grid[np.argmax(kNN_acc_scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Hyperparameter Tuning\n",
    "def LR_hyper_param(param_grid):\n",
    "    '''trains hyperparameters on a specific model, returns list of accuracy values'''\n",
    "    all_scores = []\n",
    "    for params in param_grid:\n",
    "        hyp_model = LogisticRegression(max_iter = params['max_iter'], C = params['C'], n_jobs = params['n_jobs'], penalty = 'l2')\n",
    "        \n",
    "        X_train = train[features]\n",
    "        y_train = train[target]\n",
    "        \n",
    "        train_dates = train['DATE'].unique()\n",
    "        #test_dates = test['DATE'].unique()\n",
    "\n",
    "        n_splits = 4\n",
    "        scores = []\n",
    "        models = []\n",
    "\n",
    "        splits = KFold(n_splits=n_splits, random_state=0,\n",
    "                       shuffle=True).split(train_dates)\n",
    "\n",
    "        for i, (local_train_dates_ids, local_test_dates_ids) in enumerate(splits):\n",
    "            local_train_dates = train_dates[local_train_dates_ids]\n",
    "            local_test_dates = train_dates[local_test_dates_ids]\n",
    "\n",
    "            local_train_ids = train['DATE'].isin(local_train_dates)\n",
    "            local_test_ids = train['DATE'].isin(local_test_dates)\n",
    "\n",
    "            X_local_train = X_train.loc[local_train_ids]\n",
    "            y_local_train = y_train.loc[local_train_ids]\n",
    "            X_local_test = X_train.loc[local_test_ids]\n",
    "            y_local_test = y_train.loc[local_test_ids]\n",
    "\n",
    "            X_local_train = X_local_train.fillna(0)\n",
    "            X_local_test = X_local_test.fillna(0)\n",
    "\n",
    "            hyp_model.fit(X_local_train, y_local_train)\n",
    "\n",
    "            y_local_pred = hyp_model.predict_proba(X_local_test)[:, 1]\n",
    "\n",
    "            sub = train.loc[local_test_ids].copy()\n",
    "            sub['pred'] = y_local_pred\n",
    "            y_local_pred = sub.groupby('DATE')['pred'].transform(lambda x: x > x.median()).values\n",
    "\n",
    "            score = accuracy_score(y_local_test, y_local_pred)\n",
    "            scores.append(score)\n",
    "            print(f\"Fold {i+1} - Accuracy: {score* 100:.2f}%\")\n",
    "\n",
    "        all_scores.append(np.mean(scores)*100)\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_acc_scores = LR_hyper_param(LR_grid)\n",
    "\n",
    "# optimal LR model\n",
    "LR_grid[np.argmax(LR_acc_scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using optimal parameters from previous section to create models to feed into ensemble \n",
    "DT_tuned = DecisionTreeClassifier(criterion = 'entropy', min_samples_leaf = 7, max_depth = 11, max_features = 5)\n",
    "kNN_tuned = KNeighborsClassifier(weights = 'uniform', n_neighbors = 2, n_jobs = -1, leaf_size = 10)\n",
    "LR_tuned = LogisticRegression(penalty = 'l2', n_jobs = -1, max_iter = 45, C = 0.1)\n",
    "RF_tuned = RandomForestClassifier(max_depth = 8, n_estimators = 300, max_features = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of models\n",
    "tuned_models = [ ('kNN', kNN_tuned),('RF', RF_tuned), ('Log Reg', LR_tuned), ('DT', DT_tuned), ('Naive Bayes', NB)]\n",
    "\n",
    "#creating ensemble model based on argmax voting (voting = soft)\n",
    "ensemble = VotingClassifier(tuned_models, voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "\n",
    "train_dates = train['DATE'].unique()\n",
    "test_dates = test['DATE'].unique()\n",
    "\n",
    "n_splits = 4\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "splits = KFold(n_splits=n_splits, random_state=0,\n",
    "               shuffle=True).split(train_dates)\n",
    "\n",
    "for i, (local_train_dates_ids, local_test_dates_ids) in enumerate(splits):\n",
    "    local_train_dates = train_dates[local_train_dates_ids]\n",
    "    local_test_dates = train_dates[local_test_dates_ids]\n",
    "\n",
    "    local_train_ids = train['DATE'].isin(local_train_dates)\n",
    "    local_test_ids = train['DATE'].isin(local_test_dates)\n",
    "    \n",
    "    X_local_train = X_train.loc[local_train_ids]\n",
    "    y_local_train = y_train.loc[local_train_ids]\n",
    "    X_local_test = X_train.loc[local_test_ids]\n",
    "    y_local_test = y_train.loc[local_test_ids]\n",
    "\n",
    "    X_local_train = X_local_train.fillna(0)\n",
    "    X_local_test = X_local_test.fillna(0)\n",
    "    \n",
    "    ensemble.fit(X_local_train, y_local_train)\n",
    "\n",
    "    y_local_pred = ensemble.predict_proba(X_local_test)[:, 1]\n",
    "    \n",
    "    sub = train.loc[local_test_ids].copy()\n",
    "    sub['pred'] = y_local_pred\n",
    "    y_local_pred = sub.groupby('DATE')['pred'].transform(lambda x: x > x.median()).values\n",
    "    \n",
    "    score = accuracy_score(y_local_test, y_local_pred)\n",
    "    scores.append(score)\n",
    "    print(f\"Fold {i+1} - Accuracy: {score* 100:.2f}%\")\n",
    "\n",
    "mean = np.mean(scores)*100\n",
    "std = np.std(scores)*100\n",
    "u = (mean + std)\n",
    "l = (mean - std)\n",
    "print(f'Accuracy: {mean:.2f}% [{l:.2f} ; {u:.2f}] (+- {std:.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-skearn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (local_train_dates_ids, local_test_dates_ids) in enumerate(splits):\n",
    "    local_train_dates = train_dates[local_train_dates_ids]\n",
    "    local_test_dates = train_dates[local_test_dates_ids]\n",
    "\n",
    "    local_train_ids = train['DATE'].isin(local_train_dates)\n",
    "    local_test_ids = train['DATE'].isin(local_test_dates)\n",
    "    \n",
    "    X_local_train = X_train.loc[local_train_ids]\n",
    "    y_local_train = y_train.loc[local_train_ids]\n",
    "    X_local_test = X_train.loc[local_test_ids]\n",
    "    y_local_test = y_train.loc[local_test_ids]\n",
    "\n",
    "    X_local_train = X_local_train.fillna(0)\n",
    "    X_local_test = X_local_test.fillna(0)\n",
    "    \n",
    "\n",
    "    model = AutoSklearn2Classifier(time_left_for_this_task=60*60*2,memory_limit= 1000000)\n",
    "    print(\"Start to Fit\")\n",
    "    model.fit(X_local_train,y_local_train)\n",
    "    print(\"Finished Fit\")\n",
    "\n",
    "    y_local_pred = model.predict_proba(X_local_test)[:, 1]\n",
    "    \n",
    "    sub = train.loc[local_test_ids].copy()\n",
    "    sub['pred'] = y_local_pred\n",
    "    y_local_pred = sub.groupby('DATE')['pred'].transform(lambda x: x > x.median()).values\n",
    "\n",
    "    models.append(model)\n",
    "    score = accuracy_score(y_local_test, y_local_pred)\n",
    "    scores.append(score)\n",
    "    print(f\"Fold {i+1} - Accuracy: {score* 100:.2f}%\")\n",
    "\n",
    "mean = np.mean(scores)*100\n",
    "std = np.std(scores)*100\n",
    "u = (mean + std)\n",
    "l = (mean - std)\n",
    "print(f'Accuracy: {mean:.2f}% [{l:.2f} ; {u:.2f}] (+- {std:.2f})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
